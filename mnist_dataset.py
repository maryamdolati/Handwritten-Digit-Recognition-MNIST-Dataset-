# -*- coding: utf-8 -*-
"""MNIST Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kHEWrd49SBHV1RuAfVrIdu7Vq9hsn5C-
"""

# Commented out IPython magic to ensure Python compatibility.
!pip -q install mlxtend

import numpy as np
import math
import matplotlib.pyplot as plt
import seaborn as sns
from pylab import rcParams
import random
from mlxtend.data import loadlocal_mnist

# %matplotlib inline
sns.set(style='whitegrid')
rcParams['figure.figsize'] = 12, 6
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

print(softmax(np.array([[2, 4, 6, 8]])))

epochs = 60000
inputLayerSize, hiddenLayerSize, outputLayerSize = 2, 3, 1
LR = 0.1
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([[0],[1],[1],[0]])
w_hidden = np.random.uniform(size=(inputLayerSize, hiddenLayerSize))
w_output = np.random.uniform(size=(hiddenLayerSize,outputLayerSize))

def sigmoid(x): return 1/(1 + np.exp(-x))
def sigmoid_prime(x): return x * (1 - x)

for epoch in range(epochs):
    act_hidden = sigmoid(np.dot(X, w_hidden))
    output = np.dot(act_hidden, w_output)
    error = y - output
    dZ = error * LR
    w_output += act_hidden.T.dot(dZ)
    dH = dZ.dot(w_output.T) * sigmoid_prime(act_hidden)
    w_hidden += X.T.dot(dH)

X_test = X[1]
act_hidden = sigmoid(np.dot(X_test, w_hidden))
print(np.round(np.dot(act_hidden, w_output)))

from google.colab import files
uploaded = files.upload()
path_to_text = list(uploaded.keys())[0]
text = open(path_to_text, 'rb').read().decode(encoding='utf-8')
vocab = sorted(set(text))
char2idx = {u:i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)
text_as_int = np.array([char2idx[c] for c in text])

import tensorflow as tf
seq_length = 100
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)
sequences = char_dataset.batch(seq_length+1, drop_remainder=True)

def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text

dataset = sequences.map(split_input_target)
BATCH_SIZE = 64
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)

vocab_size = len(vocab)
embedding_dim = 25
rnn_units = 1024

def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),
    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),
    tf.keras.layers.Dense(vocab_size)
  ])
  return model

model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=BATCH_SIZE)

def loss(labels, logits):
  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

model.compile(optimizer='adam', loss=loss)
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = checkpoint_dir + "/ckpt_{epoch}"
checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)
history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])

model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)
model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
model.build(tf.TensorShape([1, None]))

def generate_text(model, start_string, num_generate=400):
  input_eval = [char2idx[s] for s in start_string]
  input_eval = tf.expand_dims(input_eval, 0)
  text_generated = []
  model.reset_states()
  for i in range(num_generate):
      predictions = model(input_eval)
      predictions = tf.squeeze(predictions, 0)
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()
      input_eval = tf.expand_dims([predicted_id], 0)
      text_generated.append(idx2char[predicted_id])
  return (start_string + ''.join(text_generated))

print(generate_text(model, start_string="пе"))

import os
import matplotlib.pyplot as plt

def softmax_crossentropy_with_logits(logits, reference_answers):
    logits_for_answers = logits[np.arange(len(logits)), reference_answers]
    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits), axis=-1))
    return xentropy

def grad_softmax_crossentropy_with_logits(logits, reference_answers):
    ones_for_answers = np.zeros_like(logits)
    ones_for_answers[np.arange(len(logits)), reference_answers] = 1
    softmax_vals = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)
    return (- ones_for_answers + softmax_vals) / logits.shape[0]

class Layer(object):
    def __init__(self):
        pass
    def forward(self, input):
        return input
    def backward(self, input, grad_output):
        num_units = input.shape[1]
        d_layer_d_input = np.eye(num_units)
        return np.dot(grad_output, d_layer_d_input)

class ReLU(Layer):
    def __init__(self):
        pass
    def forward(self, input):
        return np.maximum(0, input)
    def backward(self, input, grad_output):
        relu_grad = input > 0
        return grad_output * relu_grad

class Dense(Layer):
    def __init__(self, input_units, output_units, learning_rate = 0.1):
        self.learning_rate = learning_rate
        self.weights = np.random.normal(loc=0.0, scale=np.sqrt(2/(input_units+output_units)), size=(input_units, output_units))
        self.biases = np.zeros(output_units)
    def forward(self, input):
        return np.dot(input, self.weights) + self.biases
    def backward(self, input, grad_output):
        grad_input = np.dot(grad_output, self.weights.T)
        grad_weights = np.dot(input.T, grad_output)
        grad_biases = grad_output.mean(axis=0) * input.shape[0]
        self.weights = self.weights - self.learning_rate * grad_weights
        self.biases = self.biases - self.learning_rate * grad_biases
        return grad_input

class MCP(object):
    def __init__(self):
        self.layers = []
    def add_layer(self, layer):
        self.layers.append(layer)
    def forward(self, X):
        activations = []
        inp = X
        for l in self.layers:
            activations.append(l.forward(inp))
            inp = activations[-1]
        return activations
    def train_batch(self, X, y):
        layer_activations = self.forward(X)
        layer_inputs = [X] + layer_activations
        logits = layer_activations[-1]
        y_argmax = y.argmax(axis=1)
        loss = softmax_crossentropy_with_logits(logits, y_argmax)
        loss_grad = grad_softmax_crossentropy_with_logits(logits, y_argmax)
        for layer_index in range(len(self.layers))[::-1]:
            layer = self.layers[layer_index]
            loss_grad = layer.backward(layer_inputs[layer_index], loss_grad)
        return np.mean(loss)
    def train(self, X_train, y_train, n_epochs = 25, batch_size = 32):
        train_log = []
        for epoch in range(n_epochs):
            for i in range(0, X_train.shape[0], batch_size):
                x_batch = np.array([x.flatten() for x in X_train[i:i + batch_size]])
                y_batch = np.array([y for y in y_train[i:i + batch_size]])
                self.train_batch(x_batch, y_batch)
            train_log.append(np.mean(self.predict(X_train) == y_train.argmax(axis=-1)))
            print(f"Epoch: {epoch + 1}, Train accuracy: {train_log[-1]}")
        return train_log
    def predict(self, X):
        logits = self.forward(X)[-1]
        return logits.argmax(axis=-1)

def normalize(X):
    Xn = (X - np.min(X)) / (np.max(X) - np.min(X))
    return Xn

def one_hot(a, num_classes):
    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])

from tensorflow.keras.datasets import mnist
(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = mnist.load_data()
x_train = x_train_raw.astype(np.float32)
x_test = x_test_raw.astype(np.float32)
y_train = y_train_raw.astype(int)
y_test = y_test_raw.astype(int)

def show_images(images, title_texts):
    cols = 5
    rows = int(len(images)/cols) + 1
    plt.figure(figsize=(28, 28))
    index = 1
    for img, title_text in zip(images, title_texts):
        plt.subplot(rows, cols, index)
        plt.imshow(img, cmap=plt.cm.gray)
        if title_text != '':
            plt.title(title_text, fontsize=15)
        index += 1
    plt.show()

random_images = []
for i in range(0, 10):
    r = random.randint(0, 59999)
    random_images.append((x_train[r], f"training image [{r}] = {y_train[r]}"))
for i in range(0, 5):
    r = random.randint(0, 9999)
    random_images.append((x_test[r], f"test image [{r}] = {y_test[r]}"))

show_images(list(map(lambda x: x[0], random_images)), list(map(lambda x: x[1], random_images)))

X_train = normalize(np.array([np.ravel(x) for x in x_train]))
X_test  = normalize(np.array([np.ravel(x) for x in x_test]))
Y_train = np.array([one_hot(np.array(y, dtype=int), 10) for y in y_train], dtype=int)
Y_test  = np.array([one_hot(np.array(y, dtype=int), 10) for y in y_test], dtype=int)

input_size = X_train.shape[1]
output_size = Y_train.shape[1]

network = MCP()
network.add_layer(Dense(input_size, 100, learning_rate = 0.05))
network.add_layer(ReLU())
network.add_layer(Dense(100, 200, learning_rate = 0.05))
network.add_layer(ReLU())
network.add_layer(Dense(200, output_size))

train_log = network.train(X_train, Y_train, n_epochs = 10, batch_size = 64)
plt.plot(train_log, label='train accuracy')
plt.legend(loc='best')
plt.grid()
plt.show()

test_corrects = np.sum(network.predict(X_test) == Y_test.argmax(axis=-1))
test_all = len(X_test)
test_accuracy = test_corrects / test_all
print(f"Test accuracy = {test_corrects}/{test_all} = {test_accuracy}")

def visualize_input(img, ax):
    ax.imshow(img, cmap='gray')
    width, height = img.shape
    thresh = img.max()/2.5
    for x in range(width):
        for y in range(height):
            ax.annotate(str(round(img[x][y],2)), xy=(y,x),
                        horizontalalignment='center',
                        verticalalignment='center',
                        color='white' if img[x][y]<thresh else 'black')

fig = plt.figure(figsize = (12,12))
ax = fig.add_subplot(111)
visualize_input(X_test[1:2].reshape(28,28), ax)
plt.show()